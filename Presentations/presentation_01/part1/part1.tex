\subsection{Random and Pseudo-Random Numbers}
    \begin{frame}{Random and Pseudo-Random Numbers}
        \begin{block}{A. N. Kolmogorov -- <<On Logical Foundations of Probability Theory>>}
            \it In everyday language we call random these phenomena where we cannot find a regularity allowing us to predict precisely their results. Generally speaking there is no ground to believe that a random phenomenon should possess any definite probability. Therefore, we should have distinguished between randomness proper
            (as absence of any regularity) and stochastic randomness (which is the subject of the probability theory).

            ...

            Since randomness is defined as absence of regularity, we should
            primarily specify the concept of regularity. The natural means of such a specification is the theory of algorithms and recursive functions...
        \end{block}
        Check out the \href{https://youtu.be/qKVoFqp1DzA}{lecture by A. N. Shiryaev} for more details.
    \end{frame}

    \begin{frame}{Random and Pseudo-Random Numbers}{Collins Dictionary Definitions}
        \begin{definition}[\href{https://www.collinsdictionary.com/dictionary/english/random-numbers}{Random numbers}]
            a sequence of numbers that do not form any progression, used to facilitate unbiased sampling of a population.
        \end{definition}
        \begin{definition}[\href{https://www.collinsdictionary.com/dictionary/english/pseudorandom}{Pseudorandom numbers}]
            a sequence of numbers that satisfies statistical tests for randomness but is produced by a reproducible mathematical procedure.
        \end{definition}
    \end{frame}

\subsection{LLN and CLT}
    \begin{frame}{Law of Large Numbers}
        \begin{theorem}[Khinchin]
            Let $X_1, X_2, \dots, X_n$ be a sequence of independent and identically distributed random variables with $\E X_i = \mu$. Then
            \begin{equation}
                \plim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n X_i = \mu.
            \end{equation}
        \end{theorem}
        \begin{theorem}[Kolmogorov]
            Let $X_1, X_2, \dots, X_n$ be a sequence of independent and identically distributed random variables. Then $\exists \E X_i = \mu$, if and only if
            \begin{equation}
                \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n X_i \overset{\text{a.s.}}{=} \mu.
            \end{equation}
        \end{theorem}
    \end{frame}

    \begin{frame}{Central Limit Theorem}
        \begin{theorem}[Lindeberg-L\'evy]
            Let $X_1, \dots, X_n$ be a sequence of i.i.d. random variables with $\mathbb{E}[X_i] = \mu$ and $\var\left[X_i\right] = \sigma^2$. 
            Then as $n$ approaches infinity, the random variables $\sqrt{n}(\bar{X}_n - \mu)$ converge in law to a normal distribution $\cN(0, \sigma^2)$, i.e.
            \begin{equation}
                \sqrt{n}\left(\bar{X}_n - \mu\right) \xrightarrow{d} \cN\left(0,\sigma^2\right).
            \end{equation}
        \end{theorem}
        \begin{nb}
            Law of large numbers is an informal corollary of the central limit theorem.
        \end{nb}
    \end{frame}

\subsection{Monte Carlo Simulation}
    \begin{frame}{Monte Carlo Simulation}{Statistical Estimation}
        \begin{lemma}
            Let $X_1, X_2, \dots, X_n$ be a series of independent and identically distributed random variables, and $h: \mathbb{R} \to \mathbb{R}$ be a borel function. Then $h(X_1), h(X_2), \dots, h(X_n)$ is a series of independent and identically distributed random variables.
        \end{lemma}
        Thus, we could write an unbiased consistent estimator of $\E \left[h(X)\right]$ as follows:
        \begin{equation}
            \widehat{\E \left[h(X)\right]} = \frac{1}{n} \sum_{i=1}^n h(X_i).
        \end{equation}
    \end{frame}

    \begin{frame}{Monte Carlo Simulation}{Local Truncation Error}
        \begin{definition}
            Monte Carlo simulation is a set of techniques that use pseudorandom number generators to solve problems that might be too complicated to be solved analytically. It is based on the central limit theorem.
        \end{definition}
        Asymptotic confidence interval for $\hat{\mu} = \widehat{\E\left[X\right]}$ at the confidence level $\alpha$:
        \begin{equation}
            \mu \in \left(\hat{\mu} - z_{\alpha/2} \sqrt{\frac{\sigma^2}{n}}, \hat{\mu} + z_{\alpha/2} \sqrt{\frac{\sigma^2}{n}}\right).
        \end{equation}
        That means that $LTE = 2z_{\alpha/2} \sqrt{\frac{\sigma^2}{n}}$.
    \end{frame}