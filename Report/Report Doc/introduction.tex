One of the first diffusion-based models in mathematical finance was introduced in 
\citeyear{BlackSholes1973} in the paper by Fisher Black and Myron Sholes \cite{BlackSholes1973}. 
However, the model was not very realistic, as it did not take into account the variability of
the volatility process, which was proven not to be a constant in the real stock market.
The implied volatility of the stock options was not the same for different maturities and
strikes.

Later, the class of so-called local volatility models was developed (Dupire et. al.). They
fixed the problem of the spot implied volatility: now we could get a perfect fin into the spot 
prices of the options. However, the local volatility models give us the wrong dynamics,
which is crucial to valuate the price of different derivatives.

In 1993, Steven Heston introduced a new diffusion-based model \cite{Heston1993}, but he made a 
vital assumption: the variance process is not a constant, not a determenistic function of time and 
stock price, but follows a diffusion process, called the Cox-Ingersol-Ross (CIR) process. The 
stochastic volatility models cannot be perfectly calibrated to fit the volatility smile, but
they give us a realistic dynamics of the implied volatility surface.

In this paper we revise the Heston model and its most popular simulation methods. We remind
the reader of some basic facts abou the Monte-Carlo methods in finance. We also study the 
empirical speed of convergence of the simulation methods and the accuracy of the option 
greeks. Futhermore, we implement a multi-threaded versions of the desired simulation teqniques 
and optimize them for the best possible performance in \textbf{Python}.

We provide the reader with the code for the simulation methods and the greeks computation for the 
results to be reproductible.