\chapter{A review of the original Heston model}
    \section{Basic facts}
        We shall use the following resources in this chapter: \cite{Heston1993} and \cite{Gatheral2012}.
        Assume that the spot asset's price $S$ at time $t$ follows the diffusion \eqref{Heston:price} -- \eqref{Heston:variance}:
        \begin{align}
            dS(t) & = \mu S(t)dt + \sqrt{v(t)} S(t) dZ_1(t), \label{Heston:price}\\
            dv(t) & = \left(\delta^2 - 2\beta v(t)\right) dt + 2\delta \sqrt{v(t)} dZ_2(t), \label{Heston:variance}
        \end{align}
        where $Z_1$, $Z_2$ are the correlated Wiener processes with $dZ_1dZ_2 = \rho dt$.
    \section{PDEs}
    \section{A closed-form solution for the European call option}
\chapter{A review of the Monte-Carlo methods for diffusions}
    \section{Randomness in Probability Theory}
        A. N. Kolmogorov in <<On Logical Foundations of Probability Theory>>: 
        
        \textit {In everyday language we call random these phenomena where we cannot find a regularity allowing us to predict precisely their results. Generally speaking there is no ground to believe that a random phenomenon should possess any definite probability. Therefore, we should have distinguished between randomness proper
        (as absence of any regularity) and stochastic randomness (which is the subject of the probability theory).
        Since randomness is defined as absence of regularity, we should
        primarily specify the concept of regularity. The natural means of such a specification is the theory of algorithms and recursive functions...}

        Check out the \href{https://youtu.be/qKVoFqp1DzA}{lecture by A. N. Shiryaev} for more details.

    \section{Laws of Large Numbers and Central Limit Theorem}
        \begin{theorem}[Khinchin]
            Let $X_1, X_2, \dots, X_n$ be a sequence of independent and identically distributed random variables with $\E X_i = \mu$. Then
            \begin{equation}
                \plim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n X_i = \mu.
            \end{equation}
        \end{theorem}
        \begin{theorem}[Kolmogorov]
            Let $X_1, X_2, \dots, X_n$ be a sequence of independent and identically distributed random variables. Then $\exists \E X_i = \mu$, if and only if
            \begin{equation}
                \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n X_i \overset{\text{a.s.}}{=} \mu.
            \end{equation}
        \end{theorem}

        \begin{theorem}[Lindeberg-L\'evy]
            Let $X_1, \dots, X_n$ be a sequence of i.i.d. random variables with $\E[X_i] = \mu$ and $\var X_i = \sigma^2$. 
            Then as $n$ approaches infinity, the random variables $\sqrt{n}(\bar{X}_n - \mu)$ converge in law to a normal distribution $\cN(0, \sigma^2)$, i.e.
            \begin{equation}
                \sqrt{n}\left(\bar{X}_n - \mu\right) \xrightarrow{d} \cN\left(0,\sigma^2\right).
            \end{equation}
        \end{theorem}

        Law of large numbers is an informal corollary of the central limit theorem.



    \section{The Statistical Foundations of the Monte-Carlo Methods}
        \begin{lemma}
            Let $X_1, X_2, \dots, X_n$ be a series of independent and identically distributed random variables, and $h: \mathbb{R} \to \mathbb{R}$ be a borel function. Then $h(X_1), h(X_2), \dots, h(X_n)$ is a series of independent and identically distributed random variables.
        \end{lemma}
        Thus, we could write an unbiased consistent estimator of $\E \left[h(X)\right]$ as follows:
        \begin{equation}
            \widehat{\E \left[h(X)\right]} = \frac{1}{n} \sum_{i=1}^n h(X_i).
        \end{equation}
        \begin{definition}
            Monte Carlo simulation is a set of techniques that use pseudorandom number generators to solve problems that might be too complicated to be solved analytically. It is based on the central limit theorem.
        \end{definition}
        Asymptotic confidence interval for $\hat{\mu} = \widehat{\E\left[X\right]}$ at the confidence level $\alpha$:
        \begin{equation}
            \mu \in \left(\hat{\mu} - z_{\alpha/2} \sqrt{\frac{\sigma^2}{n}}, \hat{\mu} + z_{\alpha/2} \sqrt{\frac{\sigma^2}{n}}\right).
        \end{equation}
        That means that the estimation error is equal to $2z_{\alpha/2} \sqrt{\frac{\sigma^2}{n}}$.


    \section{General Monte-Carlo Methods for Gaussian Diffusions}

    \section{The Main Methods}
        \subsection{Euler-Maruyama}
            \subsubsection{Forward Euler Scheme for ODEs}
                Suppose that we have an ODE of the form
                \begin{equation}
                    dX(t) = f(X(t), t)dt, \quad X(0) = X_0. \label{eq:ode1}
                \end{equation}
                Then it could be numerically solved by the following finite difference scheme:
                \begin{equation}
                    X_{n+1} = X_n + f(t_n, X_n)h_n, \label{Euler:ODE}
                \end{equation}
                where $t_n = \sum_{k=1}^n h_n, t_0 = 0$ is a grid. 

            \subsubsection{Backward Euler Scheme for ODEs}
                Suppose that we have an ODE of the form
                \begin{equation}
                    dX(t) = f(X(t), t)dt, \quad X(0) = X_0. \label{eq:ode2}
                \end{equation}
                Then it could be numerically solved by the following finite difference scheme:
                \begin{equation}
                    X_{n+1} = X_n + f(t_{n+1}, X_{n+1})h_n, \label{Backward:Euler:ODE}
                \end{equation}
                where $t_n = \sum_{k=1}^n h_n, t_0 = 0$ is a grid.

            \subsubsection{Euler-Maruyama Scheme for SDEs}
                Suppose we have a diffusion of the form 
                \begin{equation*}
                    dX(t) = f(X(t), t)dt + \sigma(X(t), t)dW(t), \quad X_0 = X_0.
                \end{equation*}
                Then it could be numerically solved by the following finite difference scheme:
                \begin{equation}
                    X_{n+1} = X_n + f(t_n, X_n)h_n + \sigma(t_n, X_n) \sqrt{h_n} Z_n, \label{Euler:SDE}
                \end{equation}
                where $(Z_n)_{n=1, 2, \dots}$ is a sample of standard normal random variables, and $t_n = \sum_{k=1}^n h_n, t_0 = 0$ is a grid.
                The same method could be generalized for the two-factor Gaussian diffusions. Further we assume
                that $(t_i)_{i = 0, 1, \dots}$ is a uniform grid with $t_i = ih$.

                \begin{definition}
                    Let $\hat X^n(t)$ be a piecewise mesh approximation of an SDE solution $X(t)$ (we assume that there exists a unique strong solution). 
                    Then a scheme is said to have a strong convergence of order $p$ if 
                    \begin{equation}
                        \E\left[\left|\hat X^n(T) - X(T)\right|\right] \leq Ch^p, \quad n \to \infty.
                    \end{equation}
                    A scheme is said to have a weak convergence of order $p$ if for any polynomial $f: \R \to \R$ we have
                    \begin{equation}
                        \left|\E\left[f(\hat X^n(T))\right] - \E\left[f(X(T))\right]\right| \leq Ch^p, \quad n \to \infty.
                    \end{equation}
                \end{definition}

                \begin{theorem}
                    Under some technical assumptions the Euler-Maruyama scheme \eqref{Euler:SDE} has a strong convergence of order $1/2$ and a weak convergence of order $1$.
                \end{theorem}
            
                Since our goal is to approximate $\E\left[h(X)\right]$ with a given accuracy and the least possible number of simulations, we need to compare the weak convergence rate between the methods.
                
                

\chapter{The Three Methods of Simulation of the Heston Model}
    \section{Euler Scheme}
        Suppose we have the Heston model \eqref{Heston:price} -- \eqref{Heston:variance}. Then it could be numerically solved by the following finite difference scheme:
        \begin{align}
            S_{n+1} & = S_n + \mu S_n h_n + \sqrt{v_n} S_n \sqrt{h_n} Z_{1,n}, \label{Euler:Heston:price}\\
            v_{n+1} & = v_n + \left(\delta^2 - 2\beta v_n\right) h_n + \sigma \sqrt{v_n} \sqrt{h_n} Z_{2,n}, \label{Euler:Heston:variance}
        \end{align}
        where $(Z_{1,n})_{n=1, 2, \dots}$ and $(Z_{2,n})_{n=1, 2, \dots}$ are $\rho$-correlated samples of standard normal random variables, and $t_n = \sum_{k=1}^n h_n$ is a mesh grid.
        But we have a problem: during simulation of the Heston model using Euler method $S_{t_n}$ and $v_{t_n}$ could be negative. How do we deal with this inconvenience?
        Let us introduce the log-prices
        \begin{equation}
            X(t) := \log\frac{S(t)}{S(0)}.
        \end{equation}
        We take the positive part of the variance:
        \begin{align}
            X_{n+1} & = X_n + (\mu - 0.5 v_n^+)h_n + \sqrt{v_n^+} X_n \sqrt{h_n} Z_{1,n}, \label{Euler:Heston:price:posmod}\\
            v_{n+1} & = v_n + \left(\delta^2 - 2\beta v_n^+\right) h_n + \sigma \sqrt{v_n^+} \sqrt{h_n} Z_{2,n}, \label{Euler:Heston:variance:posmod}
        \end{align}
        and then we take the exponential of the log-prices:
        \begin{equation}
            S_{n} = S_0 e^{X_{n}}.
        \end{equation}
        
        However, the scheme is not accurate, since we ignore the $dZ_idZ_j$ terms in the It\^o-Taylor series approximation.

    \section{Broadie-Kaya Scheme}
    It follows from Heston model that for $t > u$
    \begin{align}
         S_t &= S_u e^{\left( r(t-u)-\frac{1}{2} \int_{u}^{t} v_s \, ds  + \rho\int_{u}^{t} \sqrt{v_s} \, dZ_1(s) + (1-\rho)\int_{u}^{t} \sqrt{v_s} \, dZ_2(s)  \right)}, \label{BK:Price_int} \\
         v_t &= v_u + \kappa\theta(t-u) - \kappa \int_{u}^{t} v_s \, ds + \sigma\int_{u}^{t} \sqrt{v_s} \, dZ_2(s), \label{BK:Vol_int}
    \end{align}
    Exact simulation algorithm for the Heston model:
    \begin{enumerate}
        \item Generate a sample from the distribution of $v_t$ given $v_u$;
        \item Generate a sample from the distribution of $\int_{u}^t V_s ds$ given $v_t$ and $v_u$;
        \item Recover $\int_{u}^t \sqrt{v_s} dZ_1(s)$ given $v_t$, $v_u$, and $\int_{t}^u v_s ds$;
        \item Generate a sample from the distribution of $S_t$ given $\int_{u}^t \sqrt{v_s} dZ_1(s)$ , $\int_{u}^t \sqrt{v_s} dZ_2(s)$ and $\int_{u}^t v_s ds$.
    \end{enumerate}

    \subsection*{Step 1: Generate a sample from the distribution of $v_t$ given $v_u$}
        As shown in {\color{red}Cox et al. (1985) ADD CITING TO BIB FILE} the distribution of $v_t$ given $v_u$ for some $u < t$ is, up to a scale factor, a noncentral chi-squared distribution. The transition law of $v_t$ can be expressed as:
        \begin{equation}
            v_t = \frac{\sigma^2(1-e^{-\kappa(t-u)})}{4\kappa}\chi_d'^{2}\left(\frac{4\kappa e^{-\kappa(t-u)}}{\sigma^2(1-e^{-\kappa(t-u)})} v_u  \right), \quad t > u, \label{BK:vol_law}
        \end{equation}
        where $\chi_d'^{2}(\lambda)$ denotes the noncentral chi-squared random
        variable with $d$ degrees of freedom, and noncentrality
        parameter $\lambda$, and
        \begin{equation}
            d = \frac{4\theta\kappa}{\sigma^2} \label{BK:vol_law:parameter}.
        \end{equation}
        Thus, we can sample from the distribution of $v_t$ exactly,
        provided that we can sample from the noncentral chisquared distribution.
        {\color{red}Johnson et al. (1994) ADD CITING TO BIB FILE} show that for $d > 1$, the following representation is valid:
        \begin{equation}
            \chi_d'^{2}(\lambda) = \chi_1'^{2}(\lambda) + \chi_{d-1}'^{2} = N(\lambda, 1)^2 + \chi_{d-1}^{2}.
        \end{equation}
        Therefore, when $d > 1$, sampling from a noncentral chi-squared distribution
        is reduced to sampling from an ordinary chi-squared and
        an independent normal.
        When $d < 1$ we can use the the fact that
        \begin{equation}
            \chi_d'^{2}(\lambda) \sim \chi_{d + 2N}^{2},
        \end{equation}
        where $N$ is a Poisson random variable with mean $\frac{\lambda}{2}$.

    \subsection*{Step 2: Generate a sample from the distribution of $\int_{u}^t V_s ds$ given $v_t$ and $v_u$}
        The folowing formula can be derived. {\color{red}The derivation could be found in in the original paper. DERIVE HERE}
        \begin{multline}
            \phi(a) = \E\left[  \exp{(ia\int_{u}^t V_s ds)} \Bigg| v_u, v_t\right] = \frac{\gamma(a)e^{-(1/2)(\gamma(a) - \kappa)(t - u)}}{\kappa(1-e^{-\gamma(a)(t-u)})} \\
            \exp{\left(\frac{v_u + v_t}{\sigma^2} \left[  \frac{\kappa(1+e^{-\kappa(t-u)})}{1-e^{-\kappa(1-u)}}                 \right]     \right)}
            \frac{I_{0.5d-1}(\sqrt{v_uv_t}\frac{4\gamma(a)e^{-0.5\gamma(a)(t-u)}}{\sigma^2(1-e^{-\kappa(a)(t-u)})})}{I_{0.5d-1}(\sqrt{v_uv_t}\frac{4\kappa e^{-0.5\kappa(t-u)}}{\sigma^2(1-e^{-\kappa(t-u)})})},
        \end{multline}
        where $\gamma(a) = \sqrt{\kappa^2 - 2\sigma i a}$ and $I_{0.5d-1}$ is a modified  Bessel function of the first kind.

        Let $V(u,t)$  denote the random
        variable that has the same distribution as the integral $\int_{u}^t V_s ds$ conditional on $v_u$ and $v_t$.    
        Then we need to invert the characteristic function to get the cumulative distribution function
        \begin{multline}
            F(x) = P(V(u, t) \leq x) = E\left[ e^{iaV(u,t)} \Big| v_u, v_t\right] =\\= \frac{1}{\pi} \int_{-\infty}^\infty \frac{\sin ux}{u} \Phi(u) du 
            = \frac{2}{\pi} \int_{0}^\infty \frac{\sin ux}{u} \Phi(u) du.
        \end{multline}
        To calculate the integral the trapezoidal rule is being used, {\color{red} ??????????? because the errors tend to annihilate for periodic and other oscillating integrands}
        \begin{equation}\label{BK:Vut_Law}
            P(V(u, t) \leq X) = \frac{hx}{\pi} + \frac{2}{\pi} \sum_{j=1}^\infty \frac{\sin hjx}{j} Re[\Phi(hj)] - e_d(h),
        \end{equation}
        where $h$ is a grid size and $e_d(h)$ is the discretization error $e_d$.
        It can be bounded above by using a Poisson summation formula:
        \begin{equation}\label{BK:Vut_Law:discretization_error}
            0 \leq e_d(h) = \sum_{k=1}^\infty\left[ F\left(\frac{2k\pi}{h} + x\right) - F\left(\frac{2k\pi}{h} - x\right)\right] \leq 1 - F\left(\frac{2\pi}{h} - x\right).
        \end{equation}
        If we want to achieve a discretization error $\alpha$, then the
        step size should be
        \begin{equation}
            h = 2\frac{2\pi}{x+ u_\alpha} \geq \frac{\pi}{u_\alpha},
        \end{equation}
        where $1-F(u_\alpha) = \alpha$ and $0 \leq x \leq u_\alpha$.
        To be able to calculate $P(V(u, t) < x )$ using \eqref{BK:Vut_Law}, we
        need to determine a point at which the summation can be
        terminated. Let $N$ represent the last term to be calculated
        so that the approximation becomes
        \begin{equation}
            F(x) = P(V(u, t) \leq X) = \frac{hx}{\pi} + \frac{2}{\pi} \sum_{j=1}^N \frac{\sin hjx}{j} \Re[\Phi(hj)] - e_d(h) - e_T(N).
        \end{equation}
        Because $|\sin(ux)| \leq 1$, the integrand in \eqref{BK:Vut_Law:discretization_error} is bounded by
        \begin{equation}
            \frac{2|\Re[\Phi(u)]|}{\pi u} \leq \frac{2|\Phi(u)|}{\pi u}.
        \end{equation}
        To simulate the value of the integral, the Smirnov's transform method is used. We generate a uniform random variable U and then find the value of x for which 
        \begin{equation}
            P(V(u, t) \leq x) = U.
        \end{equation}

    \subsection*{Step 3: Generate a sample from the distribution of $V(u, t)$ given $v_u$ and $v_t$}
        The following formula can be used to calculate $\int_{u}^t \sqrt{v_s} dZ_1(s)$, as we already generated samples for $v_t , v_u, V(u, t)$
        \begin{equation}
            \int_{u}^t \sqrt{v_s} dZ_1(s) = \frac{1}{\sigma}(v_t v_u) - \kappa\theta(t-u) + V(u, t).
        \end{equation}


    \subsection*{Step 4: Generate a sample from the distribution of $V(u, t)$ given $v_u$ and $v_t$}
        Lastly, we need to bring everything together:
        \begin{itemize}
            \item $\int_{u}^t \sqrt{v_s} dZ_1(s)$ and $\int_{u}^t \sqrt{v_s} dZ_2(s)$ are already calculated;
            \item $V(u, t) = \int_{u}^t v_s ds $ is also calculated.
        \end{itemize}
        \begin{equation}
            S_t = S_u \exp{\left( r(t-u)-\frac{1}{2} V(u, t)  + \rho\int_{u}^{t} \sqrt{v_s} \, dZ_1(s) + (1-\rho)\int_{u}^{t} \sqrt{v_s} \, dZ_2(s)  \right)}
        \end{equation}

    \section{Andersen Scheme}
        Motivation for these schemes is the following two facts:
        \begin{itemize}
            \item Euler scheme is not very accurate, but fast and easy to implement;
            \item Broadie-Kaya scheme is more accurate, but significantly slower and way more complicated.
        \end{itemize}
        \subsection{Quadratic-Exponential Discretization Scheme}
            We denote 
            \begin{align}
                m    &= \E\left[\left.\hat{V}(t+\Delta)\right| \hat{V}(t)\right], \label{Andersen:mean}\\
                s^2  &= \E\left[\left.\left(\hat{V}(t+\Delta) - m\right)^2\right| \hat{V}(t)\right], \\
                \psi &= \frac{s^2}{m^2}.\label{Andersen:CV}
            \end{align}
            Andersen proposes an approximation based on moment-matching techniques. His goal is then to speed up the first step of Broadie and Kaya's method.
            He observes that the conditional distribution of $\hat{V}(t+\Delta)$ given $\hat{V}(t)$ visually difers when $\hat{V}(t)$ is small or large (in the variation coefficient sense).
            The scheme is constructed from the following two subschemes:
            \begin{enumerate}
                \item Quadratic sampling scheme ($\psi \leq 2$);
                \item Exponential sampling scheme ($\psi \geq 1$).
            \end{enumerate}
            Fortunately, these two intervals cover the whole positive real line. Furthermore, these two schemes could be applied at the same time when $\psi\in[1, 2]$. This implies that there exists a critical value $\psi_{\text{crit}}\in[1, 2]$, which could be an indicator of which scheme is more applicable at the given value of $\psi$. Let us show you this.
            \subsubsection{Quadratic Sampling Scheme}
                For large enough $\hat{V}(t)$ (in the $CV$-sense) we can approximate the distribution of $\hat{V}(t+\Delta)$ by the scaled non-central chi-squared distribution with $1$ degree of freedom:
                \begin{align}
                    \law\left(\left.\hat{V}(t+\Delta) \right| \hat V(t)\right) =  a(\Delta, \hat{V}(t), VP) \chi'^2_1(b(\Delta, \hat{V}(t), VP)),
                \end{align}
                where $VP$ is the vector of parameters of the CIR variance.
                \begin{lemma}
                    We have
                    \begin{align}
                        b^2 &= \frac{2}{\psi} -1 +\sqrt{\frac{2}{\psi}\left(\frac{2}{\psi}-1\right)}, \\ 
                        a   &= \frac{m}{1+b^2}.
                    \end{align}
                \end{lemma}
                \begin{proof}
                    {\color{red}Plain equating of the theoretical and real moments.}
                \end{proof}
                \textbf{Remark:} The above lemma is not valid for $\psi \geq 2$.
                Therefore, if $\hat{V}(t)$ is close to zero, then we have a problem in finding such $a = a(\Delta, \hat{V}(t), VP)$ and $b = b(\Delta, \hat{V}(t), VP)$ such that the moments of the desired conditional distribution could be properly matched.

            \subsubsection{Exponential Sampling Scheme}
                Therefore, we approximate the desired distribution with the following method. Let $\xi$ and $\eta$ be independent random variables and  $\xi \sim Be(1-p)$, $\eta \sim Exp(\beta)$ for some $p \in (0, 1)$ and $\beta > 0$. Then we have (given $\hat{V}(t)$)
                \begin{equation}
                    \hat{V}(t+\Delta) = \xi\cdot\eta,
                \end{equation}
                what gives us the following distribution density:
                \begin{equation}
                    p_{\hat{V}(t+\Delta)\vert \hat{V}(t)} = p\cdot \delta(x) + (1-p) \cdot\beta e^{-\beta x},
                \end{equation}
                where $\delta(x)$ is a standart delta function and for some $\beta$ and $p$.
                Sampling $\xi$ and $\eta$: Smirnov's transform. Or we can use the Smirnov transform with the cdf of the desired distribution.
                \begin{lemma}
                    We have
                    \begin{equation}
                        p     = \frac{\psi - 1}{\psi + 1}, \qquad \beta = \frac{1-p}{m} = \frac{2}{m(\psi+1)}.
                    \end{equation}
                \end{lemma}
                \begin{proof}
                    {\color{red}By direct integration of the given densities we get the following:}
                    \begin{equation}
                        \frac{1-p}{\beta} = m, \qquad \frac{1-p^2}{\beta^2} = s^2.
                    \end{equation}
                \end{proof}
                \begin{remark}
                    The above lemma is not valid for $\psi \leq 1$.
                \end{remark}

        \subsection{Truncated Gaussian Discretization Scheme}
            The main idea of the method: in this scheme the idea is to sample from a moment-matched Gaussian density where all probability
            mass below zero is inserted into a delta-function at the origin. Formalization of the idea:
            \begin{equation}
                \left(\left.\hat{V}(t+\Delta)\right| V(t)\right) = \left(\mu + \sigma Z\right)^+,
            \end{equation}
            where $Z$ is a standard normal random variable and $\mu$ and $\sigma$ are the 'mean' and the 'standard deviation' of the desired distribution.
            We find $\mu$ and $\sigma$ from the moment-matching techniques (see the previous method,  equations \eqref{Andersen:mean} -- \eqref{Andersen:CV}).

            \begin{lemma}
                Let $\phi(x)$ be a standart Gaussian density and define a function $r:\mathbb{R} \to \mathbb{R}$ by the following equation:
                \begin{equation}
                    r(x)\phi(r(x))+\Phi(r(x))(1+r(x)^2)= (1+x)\left(\phi(r(x)) + r(x)\Phi(r(x))\right)^2.
                \end{equation}
                Then the moment-matching parameters are
                \begin{align}
                    \mu &= \frac{m}{\frac{\phi(r(\psi))}{r(\psi)} + \Phi(r(\psi))},\\ 
                    \sigma &= \frac{m}{\phi(r(\psi)) + r(\psi)\Phi(r(\psi))}.
                \end{align}
            \end{lemma}
            \begin{proof}
                {\color{red}PROOF HERE}
            \end{proof}
            \textbf{Problem}: no closed-form solution for $r(\psi)$. 
        
        \textbf{Solution}: numerical solution.

        \textbf{Problem}: no known limits to use the numerical solution.

        \textbf{Solution}: 
        % kappa = 2\beta, theta = \frac{\delta^2}{2\beta}
        \begin{align}
            m   &= \frac{\delta^2}{2\beta} + \left(\hat{V}(t) - \frac{\delta^2}{2\beta}\right)e^{-2\beta \Delta},\\
            s^2 &= \frac{\hat{V}(t)\sigma^2e^{-2\beta \Delta}}{2\beta}\left(1 - e^{-2\beta \Delta}\right) + \frac{\delta^2\sigma^2}{8\beta^2}\left(1 - e^{-2\beta \Delta}\right)^2.
        \end{align}
        Then we analyze $\psi$ wrt $\hat{V}(t)$ and obtain a finite interval as a domain for $r(\psi)$.
        \begin{proof}
            {\color{red}PROOF HERE. Redo as a lemma}
            
        \end{proof}